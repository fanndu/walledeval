# HuggingFace LLM Support

WalledEval supports a plethora of LLM models accessible through the [HuggingFace Hub](https://huggingface.co/models). This means that any model deployed on HuggingFace under the [`text-generation`](https://huggingface.co/tasks/text-generation) task can be loaded up as a SUT.

These LLMs can be accessed via the `walledeval.llm.HF_LLM` class. Here is a quick guide to the `HF_LLM` class.


## Initiating `HF_LLM`

<font size="4px">`HF_LLM(id, system_prompt = "", type = LLMType.NEITHER)`</font>

Initiates LLM from HuggingFace Hub.

**Parameters:**

- `id` (`str`): Identifier of LLM from [HuggingFace Hub](https://huggingface.co/models). For example, [`"meta-llama/Meta-Llama-3-8B"`](https://huggingface.co/meta-llama/Meta-Llama-3-8B). Ensure that the model falls within the task of [`text-generation`](https://huggingface.co/tasks/text-generation).
- `system_prompt` (`str`, _optional_): Default System Prompt for LLM (note: this is overridden if a system prompt is provided by the user in the generation process). Defaults to an empty string.
- `type` (`int` or `LLMType`, _optional_): Type of LLM to discriminate. Integer values should fall between 0 and 2 to signify the corresponding `LLMType` value. This is overridden by the `instruct` field in `HF_LLM.generate`. By default, this value is `LLMType.NEITHER`, which means that the user needs to specify during the `HF_LLM.generate` function or use the specific functions indented for use.

## Chatting with `HF_LLM`

<font size="4px">`HF_LLM.chat(text, max_new_tokens = 256, temperature = 0.0) -> str`</font>

Uses a chat format (provided by the tokenizer) to get the LLM to complete a chat discussion.

**Parameters:**

- `text` (`Messages`): Input in either string or list format to generate LLM data. (See the above Input Types subsection for more info regarding the `Messages` type). If a system prompt is specified at the start, it is used in place of the previously specified System Prompt.
- `max_new_tokens` (`int`, _optional_): Maximum tokens to be generated by the LLM. Per LLM, there is a different range of values for this variable. Defaults to 256.
- `temperature` (`float`, _optional_): Temperature of LLM being queried. This variable is highly dependent on the actual LLM. Defaults to 0.

## Next-Token Completion

<font size="4px">`HF_LLM.complete(text, max_new_tokens = 256, temperature = 0.0) -> str`</font>

Uses LLM as a next-token predictor to generate a completion of a piece of text.

**Parameters:**
- `text` (`str`): Input in **only** string format to generate LLM data. Unlike chat completion, this does not support a chat format as an input.
- `max_new_tokens` (`int`, _optional_): Maximum tokens to be generated by the LLM. Per LLM, there is a different range of values for this variable. Defaults to 256.
- `temperature` (`float`, _optional_): Temperature of LLM being queried. This variable is highly dependent on the actual LLM. Defaults to 0.

## Generic Generation

<font size="4px">`HF_LLM.generate(text, max_new_tokens = 256, temperature = 0.0, instruct = None) -> str`</font>

Merges the `chat` and `complete` methods into a single method to simplify accessing the generation defaults.

- `text` (`Messages`): Input in either string or list format to generate LLM data. (See the above Input Types subsection for more info regarding the `Messages` type). If this is indeed a completion, any list input will throw a `ValueError`. If a system prompt is specified at the start, it is used in place of the previously specified System Prompt.
- `max_new_tokens` (`int`, _optional_): Maximum tokens to be generated by the LLM. Per LLM, there is a different range of values for this variable. Defaults to 256.
- `temperature` (`float`, _optional_): Temperature of LLM being queried. This variable is highly dependent on the actual LLM. Defaults to 0.
- `instruct` (`bool or None`, _optional_): Optional flag to change behaviour of `generate` command. This overrides the input `type` parameter at instantiation. Defaults to `None` (i.e. it uses the `type` parameter at instantiation).
