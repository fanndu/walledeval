# WalledEval


[![PyPI Latest Release](https://img.shields.io/pypi/v/walledeval.svg)](https://pypi.org/project/walledeval/)
[![PyPI Downloads](https://static.pepy.tech/badge/walledeval)](https://pepy.tech/project/walledeval)

Welcome to the WalledEval Documentation!

**WalledEval** is a simple library to test LLM safety by identifying if text generated by the LLM is indeed safe. We purposefully test benchmarks with negative information and toxic prompts to see if it is able to flag prompts of malice.

!!! note "New Version Recently Released"

    We have recently released `v0.1.0` of our codebase! This means that our documentation is not completely up-to-date with the current state of the codebase. However, we will be updating our documentation soon for all users to be able to quickstart using WalledEval! Till then, it is always best to consult the code or the `tests/` or `notebooks/` folders to have a better idea of how the codebase currently works.