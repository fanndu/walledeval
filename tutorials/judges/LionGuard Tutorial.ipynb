{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5202b9c7-562d-4c56-9a4d-fa2bb19b256c",
   "metadata": {},
   "source": [
    "# LionGuard\n",
    "\n",
    "LionGuard is a classifier for detecting unsafe content in the Singaporean context. It uses [pre-trained BAAI English embeddings](https://huggingface.co/BAAI/bge-large-en-v1.5) and performs classification with a trained classification model. The [original beta version](https://huggingface.co/jfooyh/lionguard-beta) uses an XGBoost classifier, whereas the [new LionGuard models](https://huggingface.co/dsaidgovsg) released by DSAID GovTech use a Ridge classifier.\n",
    "\n",
    "<small><i>GovTech introduced LionGuard [on 24th June at a technical sharing session](https://www.imda.gov.sg/activities/activities-catalogue/technical-sharing-session-on-ai-robustness). [LionGuard: Building a Contextualized Moderation Classifier to Tackle Localized Unsafe Content](https://arxiv.org/abs/2407.10995) was released on the same day, detailing the architecture of LionGuard and comparing it to various contemporaries. DSAID at GovTech later released a [Medium post](https://medium.com/dsaid-govtech/building-lionguard-a-contextualised-moderation-classifier-to-tackle-local-unsafe-content-8f68c8f13179) explaining the rationale and design of LionGuard. The above description has been modified from the [LionGuard-v1 model release](https://huggingface.co/govtech/lionguard-v1).</i></small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc3ad8b9-5eb5-4635-bcfa-914dd7cd5667",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce6f4fa0-4b80-4e98-8a9b-44eea583ab28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556d3f22-3ae5-4195-a3e5-c2396675d625",
   "metadata": {},
   "source": [
    "## Basic Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f99f6d5f-51f5-4429-b107-db2d1231c6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from walledeval.judge import LionGuardJudge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f82755-bf49-42d0-bc25-fa0e93059dfc",
   "metadata": {},
   "source": [
    "The [LionGuard-binary](https://huggingface.co/dsaidgovsg/lionguard-binary-v1.0) model is a Ridge classifier that embeds the input text based on the [BAAI English embeddings model](https://huggingface.co/BAAI/bge-large-en-v1.5) and inputs said embeddings into the pretrained classifier. Our system uses presets to load any of the models released by LionGuard's team.\n",
    "\n",
    "These are the models supported:\n",
    "\n",
    "|Model|Classifier Type|Preset|\n",
    "|:--------------------------------------------------------------:|:-------:|:------:|\n",
    "| [LionGuard-binary-v1.0](https://huggingface.co/dsaidgovsg/lionguard-binary-v1.0 ) | Ridge | `binary` |\n",
    "| [LionGuard-harassment-v1.0](https://huggingface.co/dsaidgovsg/lionguard-harassment-v1.0) | Ridge | `harassment` | \n",
    "| [LionGuard-hateful-v1.0](https://huggingface.co/dsaidgovsg/lionguard-hateful-v1.0) | Ridge | `hateful` |\n",
    "| [LionGuard-public_harm-v1.0](https://huggingface.co/dsaidgovsg/lionguard-public_harm-v1.0) | Ridge | `public_harm` |\n",
    "| [LionGuard-self_harm-v1.0](https://huggingface.co/dsaidgovsg/lionguard-self_harm-v1.0) | Ridge | `self_harm` |\n",
    "| [LionGuard-sexual-v1.0](https://huggingface.co/dsaidgovsg/lionguard-sexual-v1.0) | Ridge | `sexual` |\n",
    "| [LionGuard-toxic-v1.0](https://huggingface.co/dsaidgovsg/lionguard-toxic-v1.0 ) | Ridge | `toxic` |\n",
    "|[LionGuard-violent-v1.0](https://huggingface.co/dsaidgovsg/lionguard-violent-v1.0) | Ridge | `violent` |\n",
    "\n",
    "<!--| [LionGuard-beta](https://huggingface.co/jfooyh/lionguard-beta) | XGBoost | `beta` |-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c4ef989-f529-4f82-bea4-00056700e262",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4aca7335c5d4443a833e51c57b3bf3b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "models/lionguard-binary.onnx:   0%|          | 0.00/12.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<walledeval.judge.lionguard.LionGuardJudge at 0x7fa3a7a854b0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary = LionGuardJudge.from_preset(\"binary\")\n",
    "binary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c24d555-9e36-42cd-a490-e0584964bfd6",
   "metadata": {},
   "source": [
    "The `embed` function is an intermediary function used to format embeddings based on the BBAI English Embeddings model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7f131af-66c1-431c-98e8-40286280b30f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0343454 ,  0.03316101,  0.02191255, ..., -0.00717639,\n",
       "        -0.00188533,  0.01511723]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary.embed(\"Hello World\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc098da7-c6c5-417c-87c0-8d3976d09ccb",
   "metadata": {},
   "source": [
    "The `check` function is the default function in our judges, which takes in the text and outputs a logit. The logit value is a value between 0 and 1 and shows how toxic the text is (1 being \"most toxic\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76823a20-0b69-447b-95c8-859bec7332a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0007077140617184341"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary.check(\"Hello World\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e449ced-b954-4e76-8e8a-a2d202c28e59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary.check(\"bloody fuck you bloody\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bd9a8dc-4c6b-4de4-ba62-7cb771ca407a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5783132314682007"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary.check(\"knn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00de7aa2-3643-49e6-931c-485ec3c8e221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09701492637395859"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary.check(\"chibai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dcaf15e8-88df-43b5-ad43-bf0400b28fb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.920634925365448"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary.check(\"knn chao chibai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1f01222-ba7c-4adb-ad49-859f4bc842d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary.score(0.920634925365448)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a47a673-ff2a-4cae-b17d-2b4b6efe263a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: 0.920634925365448\n",
      "Classification: unsafe\n"
     ]
    }
   ],
   "source": [
    "output, score = binary(\"knn chao chibai\")\n",
    "print(\"Output:\", output)\n",
    "print(\"Classification:\", \"safe\" if score else \"unsafe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b71903c-d16c-4e0b-bf19-fd87bdc539ef",
   "metadata": {},
   "source": [
    "The above results show that the model is well-finetuned on english words like \"fuck\", but funnily enough, a chinese swear word like \"knn\" isn't strongly recognized, and another like \"chibai\" isn't even classified positively. This shows that the model is not very good at classifying toxic Singaporean phrases, which is rather ironic. However, when the content contains more words, it is classified more explicitly as a unsafe sample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc1b3d5-1050-4f32-a200-4b2ffc614dae",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Testing the Beta Model\n",
    "\n",
    "The Beta model (which has since been removed from HuggingFace Hub) is different because it is a XGBoost classiifer. Hence, here's another demo (similar to above).\n",
    "\n",
    "NOTE: DO NOT TRY THIS, THE MODEL HAS BEEN TAKEN DOWN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a3a393a-b3ac-415b-9dae-c02393d4f6f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<walledeval.judge.lionguard.LionGuardJudge at 0x7f85a6c8ceb0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta = LionGuardJudge.from_preset(\"beta\")\n",
    "beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ed0b82a-c99b-4370-a7a6-46b959ae24e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0343454 ,  0.03316101,  0.02191255, ..., -0.00717639,\n",
       "        -0.00188533,  0.01511723]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta.embed(\"Hello World\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e0c4494-0efd-4622-8ea5-af497a707cd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.457677565980703e-05"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta.check(\"Hello World\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7382c9dc-70bc-46e2-8d3e-62a3613f6c8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999995231628418"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta.check(\"bloody fuck you bloody\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "611354f6-c9c2-424a-820a-761d63617811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.999834418296814"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta.check(\"knn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ffb8dab-1405-4f15-a9de-384d5cec1eac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0005032883491367102"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta.check(\"chibai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "394439b5-0a5d-4f48-ae39-6eb6dfee552c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8807035684585571"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta.check(\"knn chao chibai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e04f569-6612-41fe-be39-a000a2849eaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta.score(0.8807035684585571)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d9a91fd-737c-4f5b-90f3-04b956ef2ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: 0.8807035684585571\n",
      "Classification: unsafe\n"
     ]
    }
   ],
   "source": [
    "output, score = beta(\"knn chao chibai\")\n",
    "print(\"Output:\", output)\n",
    "print(\"Classification:\", \"safe\" if score else \"unsafe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509a849a-ae40-476d-b35b-9cfa0e572a19",
   "metadata": {},
   "source": [
    "Even here, it acts pretty similarly, but is able to identify a word like \"knn\" is pretty bad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fe2799-8d4f-41df-9d63-f9868c77582e",
   "metadata": {},
   "source": [
    "## Testing All Models Together\n",
    "\n",
    "We can also choose to perform an analysis on how each model recognizes text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa98febc-cbae-454e-b0bb-6a2aa5b35202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91501f624f6d40ab90e5a5dc94a4d98e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "models/lionguard-toxic.onnx:   0%|          | 0.00/10.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6444abba53d459dbcd7b88d56c2ef6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "models/lionguard-violent.onnx:   0%|          | 0.00/10.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "presets = [\"binary\", \"harassment\", \"hateful\", \"public_harm\", \"self_harm\", \"sexual\", \"toxic\", \"violent\"]\n",
    "\n",
    "judges = {\n",
    "    preset: LionGuardJudge.from_preset(preset)\n",
    "    for preset in presets\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "042644c6-0d4c-4c08-b437-b9ca1673b3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def ensemble(text):\n",
    "    return {name: judge.check(text) for name, judge in judges.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "89f558c0-ed30-44ff-9432-b51cb7dad76c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'binary': 0.824999988079071,\n",
       " 'harassment': 0.30227137,\n",
       " 'hateful': 0.27736473,\n",
       " 'public_harm': 0.37700772,\n",
       " 'self_harm': 0.32861292,\n",
       " 'sexual': 0.20570648,\n",
       " 'toxic': 0.42799044,\n",
       " 'violent': 0.2513579}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble(\"fuck\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "02ae64b5-0e66-45b8-a614-1a246ec017af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'binary': 0.5783132314682007,\n",
       " 'harassment': 0.27873802,\n",
       " 'hateful': 0.46916413,\n",
       " 'public_harm': 0.39066374,\n",
       " 'self_harm': 0.5401348,\n",
       " 'sexual': 0.49440575,\n",
       " 'toxic': 0.4349258,\n",
       " 'violent': 0.43546712}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble(\"knn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "06e08350-9712-4516-b0e0-2eef12bb8d4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'binary': 0.09701492637395859,\n",
       " 'harassment': -0.63368547,\n",
       " 'hateful': -0.27831292,\n",
       " 'public_harm': -0.28430176,\n",
       " 'self_harm': -0.33843124,\n",
       " 'sexual': -0.41787767,\n",
       " 'toxic': -0.18748307,\n",
       " 'violent': -0.52312994}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble(\"chibai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5ed3c9-c41b-4172-9c29-35cdc8425c91",
   "metadata": {},
   "source": [
    "These results appear to suggest that `binary` does not seem to coincide well with the safety taxonomy laid out by the LionGuard researchers. The probability scores also somehow manage to go negative, which is very odd, assuming the output of the ridge classiifer is a probability score as the codebase does imply."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
