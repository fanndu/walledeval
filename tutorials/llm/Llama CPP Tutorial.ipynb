{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd67e7b3-b106-48f0-bf68-edd0f7ee413a",
   "metadata": {},
   "source": [
    "# `llama.cpp`\n",
    "\n",
    "In this tutorial, we will cover the `Llama` class, which is a Python implementation of [`llama.cpp`](https://github.com/ggerganov/llama.cpp), built upon [`llama-cpp-python`](https://github.com/abetlen/llama-cpp-python). We support using these LLMs to generate both **chat completions** and **completions**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb3fa681-1775-48ba-9a2a-a708ba9760ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077297e6-b75b-4fc7-8715-60cd52109c37",
   "metadata": {},
   "source": [
    "## Basic Usage\n",
    "\n",
    "We use Qwen2 0.5B to demonstrate the functionalities of our `Llama` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f2f022a-622f-489e-bd69-81e3f29fb750",
   "metadata": {},
   "outputs": [],
   "source": [
    "from walledeval.llm import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aaada194-e7d1-4841-9771-f39a44ed6407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<walledeval.llm.llama.Llama at 0x7f25dfbb8880>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = Llama.from_pretrained(\n",
    "    repo_id=\"Qwen/Qwen2-0.5B-Instruct-GGUF\",\n",
    "    filename=\"*q8_0.gguf\",\n",
    "    verbose=False\n",
    ")\n",
    "llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f6f936-b4a2-4923-8cb5-9ff38ca448fb",
   "metadata": {},
   "source": [
    "LLMs can be used for **completions**, which can basically be seen as a completion of a piece of text. Here, we should Qwen2 0.5B in action trying to answer a question (and failing miserably at that by the way). This is expected because Qwen is instruction-tuned and is not build for basic text completion. Base models are generally better in these scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5686e69b-79ef-4bd4-bab7-42ce498d1dcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5. Mercury, Venus, Earth, Mars, Jupiter, Saturn.\\nThe answer is: 5. Mercury, Venus, Earth, Mars, Jupiter, Saturn'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.complete(\"Q: Name the planets in the solar system? A: \", max_new_tokens=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1da3614-6253-45b0-8135-f43277014830",
   "metadata": {},
   "source": [
    "We can also use LLMs for **chat completions**, i.e. we can chat with LLMs. I can simply put in a text with a query and the LLM will shoot out a pretty nice answer, as a chatbot should do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e34cf051-65da-45f7-84f8-d2b2eeefb045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I am a large language model created by Alibaba Cloud. I'm called Qwen. How can I assist you today?\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.chat(\"Hi, who are you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5c57da-0a97-4bae-b220-4ec8c8a87ffe",
   "metadata": {},
   "source": [
    "Another use-case is the ability to tune the methods by which these models respond to messages. Here, we try to make the LLM a Harry Potter expert (though clearly it did not work out) and see if it can answer one of the basic questions in the lore of Harry Potter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f21c60ff-9a76-4167-b0d5-4453c346f3fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Snape betrayed Dumbledore because he was jealous of his mentor's success in the wizarding world. Voldemort had taken over Hogwarts, and Snape felt that Dumbledore's influence would be lost if he were to continue teaching at the school. He saw it as a way to gain power and control over the wizarding community."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(llm.chat([\n",
    "    {\"role\": \"system\", \"content\": \"You are an expert at Harry Potter lore, and can't stop yourself from incorporating random emojis, spells and terminology into your own speech.\"},\n",
    "    {\"role\": \"user\", \"content\": \"why did snape betray dumbledore\"}\n",
    "]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
